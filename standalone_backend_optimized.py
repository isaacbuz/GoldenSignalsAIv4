#!/usr/bin/env python3
"""
Optimized Standalone Backend for GoldenSignalsAI
Implements caching, concurrent processing, and performance optimizations
Based on standalone_backend_fixed.py with performance improvements
"""

import asyncio
import json
import logging
import random
import time
import traceback
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Any
from functools import lru_cache
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor

import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import HTMLResponse, JSONResponse
from pydantic import BaseModel
import yfinance as yf
import numpy as np
import pandas as pd
from cachetools import TTLCache
from contextlib import asynccontextmanager

# Import the cache wrapper
from cache_wrapper import cache_market_data, cache_signals

# Enhanced logging configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/backend.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Error tracking system
class ErrorTracker:
    def __init__(self):
        self.errors = []
        self.error_counts = defaultdict(int)
        self.error_patterns = defaultdict(list)
        self.max_errors = 1000

    def track_error(self, error: Exception, context: dict = None):
        """Track an error with context"""
        error_data = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'type': type(error).__name__,
            'message': str(error),
            'traceback': traceback.format_exc(),
            'context': context or {}
        }

        # Add to errors list
        self.errors.append(error_data)

        # Keep only recent errors
        if len(self.errors) > self.max_errors:
            self.errors = self.errors[-self.max_errors:]

        # Count error types
        error_key = f"{type(error).__name__}:{str(error)[:100]}"
        self.error_counts[error_key] += 1

        # Track error patterns
        self.error_patterns[type(error).__name__].append({
            'timestamp': error_data['timestamp'],
            'message': str(error),
            'context': context or {}
        })

        # Log the error
        logger.error(f"Error tracked: {type(error).__name__}: {str(error)}",
                    extra={'context': context, 'traceback': traceback.format_exc()})

    def get_error_summary(self) -> dict:
        """Get summary of tracked errors"""
        recent_errors = [e for e in self.errors if
                        (datetime.now(timezone.utc) -
                         datetime.fromisoformat(e['timestamp'].replace('Z', '+00:00'))).total_seconds() < 3600]

        return {
            'total_errors': len(self.errors),
            'recent_errors_1h': len(recent_errors),
            'error_types': dict(self.error_counts),
            'most_common_errors': sorted(self.error_counts.items(), key=lambda x: x[1], reverse=True)[:10],
            'recent_errors': self.errors[-10:] if self.errors else []
        }

# Global error tracker
error_tracker = ErrorTracker()

# Performance monitoring
class PerformanceMonitor:
    def __init__(self):
        self.request_times = defaultdict(list)
        self.endpoint_stats = defaultdict(lambda: {
            'count': 0,
            'total_time': 0,
            'avg_time': 0,
            'max_time': 0,
            'min_time': float('inf'),
            'errors': 0
        })

    def track_request(self, endpoint: str, duration: float, status_code: int):
        """Track request performance"""
        self.request_times[endpoint].append(duration)

        # Keep only recent requests (last 1000 per endpoint)
        if len(self.request_times[endpoint]) > 1000:
            self.request_times[endpoint] = self.request_times[endpoint][-1000:]

        # Update endpoint stats
        stats = self.endpoint_stats[endpoint]
        stats['count'] += 1
        stats['total_time'] += duration
        stats['avg_time'] = stats['total_time'] / stats['count']
        stats['max_time'] = max(stats['max_time'], duration)
        stats['min_time'] = min(stats['min_time'], duration)

        if status_code >= 400:
            stats['errors'] += 1

    def get_performance_summary(self) -> dict:
        """Get performance summary"""
        slow_endpoints = []
        for endpoint, times in self.request_times.items():
            if times:
                avg_time = sum(times) / len(times)
                if avg_time > 1.0:  # Slow endpoints (>1s)
                    slow_endpoints.append({
                        'endpoint': endpoint,
                        'avg_time': avg_time,
                        'max_time': max(times),
                        'request_count': len(times)
                    })

        return {
            'total_requests': sum(len(times) for times in self.request_times.values()),
            'endpoints': dict(self.endpoint_stats),
            'slow_endpoints': sorted(slow_endpoints, key=lambda x: x['avg_time'], reverse=True)[:10]
        }

# Global performance monitor
performance_monitor = PerformanceMonitor()

# Initialize optional services (done in lifespan)
data_validator = None
signal_engine = None
signal_pipeline = None
monitoring_service = None

# Cache configuration
market_data_cache = TTLCache(maxsize=1000, ttl=300)  # 5 minute TTL
signal_cache = TTLCache(maxsize=500, ttl=30)  # 30 second TTL
historical_cache = TTLCache(maxsize=200, ttl=600)  # 10 minute TTL

# Thread pool for CPU-intensive tasks
executor = ThreadPoolExecutor(max_workers=4)

# Performance monitoring
request_times = defaultdict(list)

# Lifespan context manager for startup/shutdown
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("ðŸš€ Optimized backend started with caching and batch processing")

    # Try to initialize optional services
    global data_validator, signal_engine, signal_pipeline, monitoring_service
    try:
        from src.data.data_quality_validator import DataQualityValidator
        data_validator = DataQualityValidator()
        logger.info("âœ… Data Quality Validator initialized")
    except ImportError:
        logger.debug("â„¹ï¸ Data Quality Validator not available, using direct yfinance")
        data_validator = None

    try:
        from agents.signals.integrated_signal_system import SignalGenerationEngine
        from agents.pipeline.signal_filtering_pipeline import SignalFilteringPipeline
        signal_engine = SignalGenerationEngine()
        signal_pipeline = SignalFilteringPipeline()
        logger.info("âœ… Signal Generation Engine initialized")
    except ImportError:
        logger.debug("â„¹ï¸ Signal Generation Engine not available, using legacy signal generation")
        signal_engine = None
        signal_pipeline = None

    try:
        from agents.monitoring.signal_monitoring_service import SignalMonitoringService
        monitoring_service = SignalMonitoringService()
        logger.info("âœ… Signal Monitoring Service initialized")
    except ImportError:
        logger.debug("â„¹ï¸ Signal Monitoring Service not available")
        monitoring_service = None

    # Start background tasks
    asyncio.create_task(manager.batch_sender())
    asyncio.create_task(periodic_signal_updates())

    yield

    # Shutdown
    logger.info("Shutting down optimized backend...")
    executor.shutdown(wait=False)

# Create FastAPI app
app = FastAPI(
    title="GoldenSignalsAI Optimized Backend",
    description="High-performance backend with caching and optimization",
    version="2.0.0",
    lifespan=lifespan
)

# Add middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.add_middleware(GZipMiddleware, minimum_size=1000)

# Enhanced error handling middleware
@app.middleware("http")
async def error_tracking_middleware(request: Request, call_next):
    """Enhanced error tracking and performance monitoring middleware"""
    start_time = time.time()
    response = None

    try:
        response = await call_next(request)
        process_time = time.time() - start_time

        # Track performance
        performance_monitor.track_request(
            endpoint=request.url.path,
            duration=process_time,
            status_code=response.status_code
        )

        # Add performance headers
        response.headers["X-Process-Time"] = str(process_time)
        response.headers["X-Request-ID"] = f"req_{int(time.time() * 1000)}"

        # Log slow requests
        if process_time > 2.0:
            logger.warning(f"Slow request: {request.method} {request.url.path} took {process_time:.2f}s")

        return response

    except Exception as e:
        process_time = time.time() - start_time

        # Track the error
        error_tracker.track_error(e, {
            'endpoint': request.url.path,
            'method': request.method,
            'duration': process_time,
            'client_ip': request.client.host if request.client else 'unknown'
        })

        # Track performance even for errors
        performance_monitor.track_request(
            endpoint=request.url.path,
            duration=process_time,
            status_code=500
        )

        # Return structured error response
        return JSONResponse(
            status_code=500,
            content={
                "error": "Internal Server Error",
                "message": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "request_id": f"req_{int(time.time() * 1000)}"
            }
        )

# Performance monitoring middleware (keeping the original for compatibility)
@app.middleware("http")
async def add_process_time_header(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    request_times[request.url.path].append(process_time)
    return response

# Models (same as original)
class Signal(BaseModel):
    id: str
    symbol: str
    action: str  # Changed from 'type' to 'action'
    confidence: float
    price: float
    timestamp: str
    reason: str
    indicators: Dict[str, Any]
    risk_level: str
    entry_price: Optional[float] = None
    stop_loss: Optional[float] = None
    take_profit: Optional[float] = None

class MarketData(BaseModel):
    symbol: str
    price: float
    change: float
    change_percent: float
    volume: int
    timestamp: str
    bid: Optional[float] = None
    ask: Optional[float] = None
    high: Optional[float] = None
    low: Optional[float] = None
    open: Optional[float] = None

# Cached technical indicator calculations
@lru_cache(maxsize=128)
def calculate_indicators_cached(symbol: str, prices_tuple: tuple) -> dict:
    """Calculate technical indicators with caching"""
    prices = np.array(prices_tuple)

    indicators = {}

    # RSI calculation
    if len(prices) >= 14:
        deltas = np.diff(prices)
        seed = deltas[:14]
        up = seed[seed >= 0].sum() / 14
        down = -seed[seed < 0].sum() / 14
        rs = up / down if down != 0 else 0
        indicators['rsi'] = 100 - (100 / (1 + rs))

    # SMA calculations
    if len(prices) >= 20:
        indicators['sma_20'] = np.mean(prices[-20:])
        indicators['sma_50'] = np.mean(prices[-50:]) if len(prices) >= 50 else np.mean(prices)

    # Bollinger Bands
    if len(prices) >= 20:
        sma = np.mean(prices[-20:])
        std = np.std(prices[-20:])
        indicators['bb_upper'] = sma + (2 * std)
        indicators['bb_lower'] = sma - (2 * std)
        indicators['bb_percent'] = (prices[-1] - indicators['bb_lower']) / (indicators['bb_upper'] - indicators['bb_lower'])

    return indicators

# Optimized market data fetching
@cache_market_data(ttl=300)
async def get_market_data_cached(symbol: str) -> Optional[MarketData]:
    """Get market data with caching and fallback sources"""
    # Validate symbol - prevent 'latest' and other invalid symbols
    if not symbol or symbol.lower() in ['latest', 'all', 'batch'] or len(symbol) > 10 or not symbol.replace('-', '').replace('.', '').isalnum():
        logger.warning(f"Invalid symbol for market data: {symbol}")
        return None

    try:
        # Use data validator if available
        if data_validator:
            try:
                data, source = await data_validator.get_market_data_with_fallback(symbol)
                if data is not None and not data.empty:
                    # Extract latest data point
                    latest = data.iloc[-1]

                    return MarketData(
                        symbol=symbol,
                        price=float(latest.get('Close', 0)),
                        change=float(latest.get('Close', 0)) - float(latest.get('Open', 0)),
                        change_percent=((float(latest.get('Close', 0)) - float(latest.get('Open', 0))) / float(latest.get('Open', 1)) * 100),
                        volume=int(latest.get('Volume', 0)),
                        timestamp=datetime.now(timezone.utc).isoformat(),
                        bid=float(latest.get('Close', 0)) * 0.999,
                        ask=float(latest.get('Close', 0)) * 1.001,
                        high=float(latest.get('High', 0)),
                        low=float(latest.get('Low', 0)),
                        open=float(latest.get('Open', 0))
                    )
            except Exception as e:
                logger.warning(f"Data validator failed for market data {symbol}: {e}")
                # Fall through to direct yfinance

        # Fallback to direct yfinance
        loop = asyncio.get_event_loop()
        ticker = await loop.run_in_executor(executor, yf.Ticker, symbol)

        # Try to get current info
        info = await loop.run_in_executor(executor, lambda: ticker.info)

        # Get current price
        current_price = info.get('currentPrice') or info.get('regularMarketPrice', 0)
        if current_price == 0:
            # Fallback to history
            hist = await loop.run_in_executor(
                executor,
                lambda: ticker.history(period="1d", interval="1m")
            )
            if not hist.empty:
                current_price = float(hist['Close'].iloc[-1])

        previous_close = info.get('previousClose', current_price)

        return MarketData(
            symbol=symbol,
            price=current_price,
            change=current_price - previous_close,
            change_percent=((current_price - previous_close) / previous_close * 100) if previous_close else 0,
            volume=info.get('volume', 0),
            timestamp=datetime.now(timezone.utc).isoformat(),
            bid=info.get('bid', current_price * 0.999),
            ask=info.get('ask', current_price * 1.001),
            high=info.get('dayHigh', current_price),
            low=info.get('dayLow', current_price),
            open=info.get('open', current_price)
        )

    except Exception as e:
        logger.error(f"Error fetching market data for {symbol}: {e}")
        # Return mock data as fallback
        return MarketData(
            symbol=symbol,
            price=round(random.uniform(100, 500), 2),
            change=round(random.uniform(-10, 10), 2),
            change_percent=round(random.uniform(-5, 5), 2),
            volume=random.randint(1000000, 10000000),
            timestamp=datetime.now(timezone.utc).isoformat(),
            bid=round(random.uniform(100, 500), 2) * 0.999,
            ask=round(random.uniform(100, 500), 2) * 1.001,
            high=round(random.uniform(100, 500), 2),
            low=round(random.uniform(100, 500), 2),
            open=round(random.uniform(100, 500), 2)
        )

# Cached historical data fetching
async def get_historical_data_cached(symbol: str, period: str = "1mo", interval: str = "1d") -> Optional[pd.DataFrame]:
    """Get historical data with caching and fallback sources"""
    # Validate symbol - prevent 'latest' and other invalid symbols
    if not symbol or symbol.lower() in ['latest', 'all', 'batch'] or len(symbol) > 10 or not symbol.replace('-', '').replace('.', '').isalnum():
        logger.warning(f"Invalid symbol for historical data: {symbol}")
        return None

    cache_key = f"hist:{symbol}:{period}:{interval}"

    if cache_key in historical_cache:
        return historical_cache[cache_key]

    try:
        # Use data validator if available
        if data_validator:
            try:
                data, source = await data_validator.get_market_data_with_fallback(symbol)
                if data is not None and not data.empty:
                    # Clean and validate the data
                    data = data_validator.clean_data(data)
                    historical_cache[cache_key] = data
                    logger.info(f"Got historical data for {symbol} from {source}")
                    return data
            except Exception as e:
                logger.warning(f"Data validator failed for {symbol}: {e}")
                # Fall through to direct yfinance

        # Fallback to direct yfinance
        loop = asyncio.get_event_loop()
        ticker = await loop.run_in_executor(executor, yf.Ticker, symbol)
        data = await loop.run_in_executor(
            executor,
            lambda: ticker.history(period=period, interval=interval)
        )

        if not data.empty:
            historical_cache[cache_key] = data
            return data

        # Generate mock historical data if all else fails
        logger.warning(f"Using mock historical data for {symbol}")
        dates = pd.date_range(end=pd.Timestamp.now(tz='UTC'), periods=30, freq='D')
        mock_data = pd.DataFrame({
            'Open': np.random.uniform(100, 500, len(dates)),
            'High': np.random.uniform(102, 505, len(dates)),
            'Low': np.random.uniform(98, 495, len(dates)),
            'Close': np.random.uniform(100, 500, len(dates)),
            'Volume': np.random.randint(1000000, 10000000, len(dates))
        }, index=dates)

        # Ensure logical consistency
        mock_data['High'] = mock_data[['Open', 'High', 'Low', 'Close']].max(axis=1)
        mock_data['Low'] = mock_data[['Open', 'High', 'Low', 'Close']].min(axis=1)

        return mock_data
    except Exception as e:
        logger.error(f"Error fetching historical data for {symbol}: {e}")
        # Return mock data
        dates = pd.date_range(end=pd.Timestamp.now(tz='UTC'), periods=30, freq='D')
        mock_data = pd.DataFrame({
            'Open': np.random.uniform(100, 500, len(dates)),
            'High': np.random.uniform(102, 505, len(dates)),
            'Low': np.random.uniform(98, 495, len(dates)),
            'Close': np.random.uniform(100, 500, len(dates)),
            'Volume': np.random.randint(1000000, 10000000, len(dates))
        }, index=dates)

        # Ensure logical consistency
        mock_data['High'] = mock_data[['Open', 'High', 'Low', 'Close']].max(axis=1)
        mock_data['Low'] = mock_data[['Open', 'High', 'Low', 'Close']].min(axis=1)

        return mock_data

# Optimized signal generation
@cache_signals(ttl=30)
async def generate_signals_cached(symbol: str) -> List[Signal]:
    """Generate trading signals with caching"""
    # Validate symbol - prevent 'latest' and other invalid symbols
    if not symbol or symbol.lower() in ['latest', 'all', 'batch'] or len(symbol) > 10 or not symbol.replace('-', '').replace('.', '').isalnum():
        logger.warning(f"Invalid symbol for signal generation: {symbol}")
        return []

    signals = []

    # Use new signal engine if available
    if signal_engine and signal_pipeline:
        try:
            # Generate signals using the new engine
            trading_signals = await signal_engine.generate_signals([symbol])

            # Filter signals through the pipeline
            filtered_signals = signal_pipeline.filter_signals(trading_signals)

            # Convert TradingSignal to Signal (API model)
            for ts in filtered_signals:
                signal = Signal(
                    id=ts.id,
                    symbol=ts.symbol,
                    action=ts.action,
                    confidence=ts.confidence,
                    price=ts.price,
                    timestamp=ts.timestamp.isoformat(),
                    reason=ts.reason,
                    indicators=ts.indicators,
                    risk_level=ts.risk_level,
                    entry_price=ts.entry_price,
                    stop_loss=ts.stop_loss,
                    take_profit=ts.take_profit
                )
                signals.append(signal)

            if signals:
                return signals
        except Exception as e:
            logger.error(f"Error using new signal engine: {e}")
            # Fall back to legacy generation

    # Legacy signal generation
    # Get historical data
    hist_data = await get_historical_data_cached(symbol, period="1mo", interval="1d")

    if hist_data is not None and not hist_data.empty:
        # Convert prices to tuple for caching
        prices_tuple = tuple(hist_data['Close'].values)

        # Calculate indicators using cached function
        indicators = calculate_indicators_cached(symbol, prices_tuple)

        # Get current market data
        market_data = await get_market_data_cached(symbol)

        if market_data:
            # Generate signal based on indicators
            confidence = 0.0
            signal_type = "HOLD"
            reasons = []

            # RSI signal
            if 'rsi' in indicators:
                if indicators['rsi'] < 30:
                    confidence += 0.3
                    signal_type = "BUY"
                    reasons.append("RSI oversold")
                elif indicators['rsi'] > 70:
                    confidence += 0.3
                    signal_type = "SELL"
                    reasons.append("RSI overbought")

            # Bollinger Bands signal
            if 'bb_percent' in indicators:
                if indicators['bb_percent'] < 0.2:
                    confidence += 0.2
                    reasons.append("Near lower Bollinger Band")
                    if signal_type != "SELL":
                        signal_type = "BUY"
                elif indicators['bb_percent'] > 0.8:
                    confidence += 0.2
                    reasons.append("Near upper Bollinger Band")
                    if signal_type != "BUY":
                        signal_type = "SELL"

            # Create signal if we have enough confidence
            if confidence > 0 and signal_type != "HOLD":
                signal = Signal(
                    id=f"{symbol}_{int(time.time() * 1000)}",
                    symbol=symbol,
                    action=signal_type,
                    confidence=min(confidence, 1.0),
                    price=market_data.price,
                    timestamp=datetime.now(timezone.utc).isoformat(),
                    reason="; ".join(reasons),
                    indicators=indicators,
                    risk_level="low" if confidence > 0.7 else "medium" if confidence > 0.4 else "high",
                    entry_price=market_data.price,
                    stop_loss=market_data.price * 0.98 if signal_type == "BUY" else market_data.price * 1.02,
                    take_profit=market_data.price * 1.05 if signal_type == "BUY" else market_data.price * 0.95
                )
                signals.append(signal)

    # If no signals generated, create a mock signal
    if not signals:
        actions = ["BUY", "SELL", "HOLD"]
        action = random.choice(actions)

        signal = Signal(
            id=f"{symbol}_{int(time.time() * 1000)}_{random.randint(1, 100)}",
            symbol=symbol,
            action=action,
            confidence=round(random.uniform(0.6, 0.95), 2),
            price=round(random.uniform(100, 500), 2),
            timestamp=datetime.now(timezone.utc).isoformat(),
            reason=f"Technical analysis suggests {action}",
            indicators={
                "rsi": round(random.uniform(20, 80), 2),
                "sma_20": round(random.uniform(100, 500), 2),
                "volume": random.randint(1000000, 10000000)
            },
            risk_level=random.choice(["low", "medium", "high"]),
            entry_price=round(random.uniform(100, 500), 2),
            stop_loss=round(random.uniform(95, 99), 2),
            take_profit=round(random.uniform(101, 110), 2)
        )
        signals.append(signal)

    return signals

# Batch processing for multiple symbols
async def generate_signals_batch(symbols: List[str]) -> List[Signal]:
    """Generate signals for multiple symbols concurrently"""
    # Use new signal engine for batch processing if available
    if signal_engine and signal_pipeline:
        try:
            # Generate all signals at once
            trading_signals = await signal_engine.generate_signals(symbols)

            # Filter signals through the pipeline
            filtered_signals = signal_pipeline.filter_signals(trading_signals)

            # Convert TradingSignal to Signal (API model)
            all_signals = []
            for ts in filtered_signals:
                signal = Signal(
                    id=ts.id,
                    symbol=ts.symbol,
                    action=ts.action,
                    confidence=ts.confidence,
                    price=ts.price,
                    timestamp=ts.timestamp.isoformat(),
                    reason=ts.reason,
                    indicators=ts.indicators,
                    risk_level=ts.risk_level,
                    entry_price=ts.entry_price,
                    stop_loss=ts.stop_loss,
                    take_profit=ts.take_profit
                )
                all_signals.append(signal)

            return all_signals
        except Exception as e:
            logger.error(f"Error using new signal engine for batch: {e}")
            # Fall back to legacy generation

    # Legacy batch processing
    tasks = [generate_signals_cached(symbol) for symbol in symbols]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    all_signals = []
    for result in results:
        if isinstance(result, list):
            all_signals.extend(result)

    return all_signals

# WebSocket manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.update_queue = asyncio.Queue()
        self.batch_interval = 0.5  # 500ms batching

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        logger.info(f"WebSocket connected. Total connections: {len(self.active_connections)}")

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        logger.info(f"WebSocket disconnected. Total connections: {len(self.active_connections)}")

    async def queue_update(self, data: dict):
        """Queue update for batching"""
        await self.update_queue.put(data)

    async def batch_sender(self):
        """Send updates in batches"""
        while True:
            updates = []
            deadline = time.time() + self.batch_interval

            # Collect updates for batch_interval seconds
            while time.time() < deadline:
                try:
                    timeout = deadline - time.time()
                    if timeout > 0:
                        update = await asyncio.wait_for(
                            self.update_queue.get(),
                            timeout=timeout
                        )
                        updates.append(update)
                except asyncio.TimeoutError:
                    break

            if updates and self.active_connections:
                # Send batch to all connections
                batch_data = {
                    "type": "batch_update",
                    "updates": updates,
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }

                disconnected = []
                for connection in self.active_connections:
                    try:
                        await connection.send_json(batch_data)
                    except:
                        disconnected.append(connection)

                # Remove disconnected clients
                for conn in disconnected:
                    if conn in self.active_connections:
                        self.disconnect(conn)

manager = ConnectionManager()

# Background task for periodic updates
async def periodic_signal_updates():
    """Send periodic signal updates via WebSocket"""
    await asyncio.sleep(5)  # Initial delay

    symbols = ["AAPL", "GOOGL", "MSFT", "TSLA", "NVDA", "META", "AMZN", "SPY"]

    while True:
        try:
            # Generate signals for all symbols using batch processing
            all_signals = await generate_signals_batch(symbols)

            if all_signals and manager.active_connections:
                await manager.queue_update({
                    "type": "signals",
                    "data": [s.model_dump() for s in all_signals]
                })

            # Also send market updates for a few symbols
            for symbol in symbols[:3]:
                market_data = await get_market_data_cached(symbol)
                if market_data:
                    await manager.queue_update({
                        "type": "market_update",
                        "data": market_data.model_dump()
                    })

        except Exception as e:
            logger.error(f"Error in periodic updates: {e}")

        await asyncio.sleep(30)  # Update every 30 seconds



# API Endpoints
# Add missing endpoints that frontend is calling
@app.get("/api/v1/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "version": "2.0.0",
        "services": {
            "market_data": "operational",
            "signal_generation": "operational",
            "websocket": "operational"
        }
    }

@app.get("/api/v1/market-data/status/market")
async def get_market_status():
    """Get market status"""
    now = datetime.now()
    # Simple market hours check (NYSE: 9:30 AM - 4:00 PM ET)
    market_open = now.replace(hour=9, minute=30, second=0, microsecond=0)
    market_close = now.replace(hour=16, minute=0, second=0, microsecond=0)

    is_open = market_open <= now <= market_close and now.weekday() < 5

    return {
        "market_open": is_open,
        "market_hours": {
            "open": "09:30:00",
            "close": "16:00:00"
        },
        "timezone": "America/New_York",
        "next_open": market_open.isoformat() if not is_open else None,
        "next_close": market_close.isoformat() if is_open else None
    }

@app.get("/api/v1/ai/insights/{symbol}")
async def get_ai_insights(symbol: str):
    """Get AI insights for a symbol"""
    try:
        # Get market data for analysis
        market_data = await get_market_data_cached(symbol)

        # Generate AI insights
        insights = [
            {
                "type": "bullish" if random.random() > 0.5 else "bearish",
                "confidence": round(random.uniform(0.6, 0.95), 2),
                "message": f"Technical analysis suggests {symbol} is showing strong momentum",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "priority": random.choice(["high", "medium", "low"])
            },
            {
                "type": "opportunity",
                "confidence": round(random.uniform(0.7, 0.9), 2),
                "message": f"Volume analysis indicates potential breakout for {symbol}",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "priority": "medium"
            }
        ]

        return {
            "symbol": symbol,
            "insights": insights,
            "last_updated": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting AI insights for {symbol}: {e}")
        return {
            "symbol": symbol,
            "insights": [],
            "last_updated": datetime.now(timezone.utc).isoformat(),
            "error": str(e)
        }

@app.get("/api/v1/performance/overview")
async def get_performance_overview():
    """Get performance overview"""
    return {
        "total_signals": random.randint(1000, 5000),
        "successful_signals": random.randint(800, 1200),
        "accuracy_rate": round(random.uniform(0.75, 0.92), 3),
        "average_return": round(random.uniform(0.05, 0.15), 3),
        "win_rate": round(random.uniform(0.65, 0.85), 3),
        "sharpe_ratio": round(random.uniform(1.2, 2.5), 2),
        "max_drawdown": round(random.uniform(0.05, 0.15), 3),
        "total_return": round(random.uniform(0.15, 0.45), 3),
        "active_signals": random.randint(5, 25),
        "last_updated": datetime.now(timezone.utc).isoformat()
    }

@app.get("/api/v1/agents/performance")
async def get_agents_performance():
    """Get agent performance metrics"""
    agents = [
        {
            "name": "Technical Analysis Agent",
            "accuracy": round(random.uniform(0.75, 0.90), 3),
            "signals_generated": random.randint(100, 500),
            "win_rate": round(random.uniform(0.65, 0.85), 3),
            "status": "active"
        },
        {
            "name": "Sentiment Analysis Agent",
            "accuracy": round(random.uniform(0.70, 0.85), 3),
            "signals_generated": random.randint(50, 200),
            "win_rate": round(random.uniform(0.60, 0.80), 3),
            "status": "active"
        },
        {
            "name": "Options Flow Agent",
            "accuracy": round(random.uniform(0.80, 0.95), 3),
            "signals_generated": random.randint(30, 150),
            "win_rate": round(random.uniform(0.70, 0.90), 3),
            "status": "active"
        }
    ]

    return {
        "agents": agents,
        "overall_performance": {
            "average_accuracy": round(sum(a["accuracy"] for a in agents) / len(agents), 3),
            "total_signals": sum(a["signals_generated"] for a in agents),
            "average_win_rate": round(sum(a["win_rate"] for a in agents) / len(agents), 3)
        },
        "last_updated": datetime.now(timezone.utc).isoformat()
    }

@app.get("/")
async def root():
    return {
        "message": "GoldenSignalsAI Optimized Backend",
        "version": "2.0.0",
        "status": "operational",
        "features": [
            "Caching enabled (5min market data, 30s signals)",
            "Batch processing for concurrent requests",
            "Response compression (gzip)",
            "Performance monitoring",
            "WebSocket batching (500ms)"
        ]
    }

@app.get("/api/v1/signals")
async def get_all_signals():
    """Get all signals with batch processing"""
    symbols = ["AAPL", "GOOGL", "MSFT", "TSLA", "NVDA", "META", "AMZN", "SPY"]
    all_signals = await generate_signals_batch(symbols)
    return all_signals

@app.get("/api/v1/signals/latest")
async def get_latest_signals(limit: int = Query(10, ge=1, le=50)):
    """Get the latest signals across all symbols"""
    logger.info(f"DEBUG: get_latest_signals called with limit={limit}")
    symbols = ["AAPL", "GOOGL", "MSFT", "TSLA", "NVDA", "META", "AMZN", "SPY"]
    all_signals = await generate_signals_batch(symbols)

    # Sort by timestamp (newest first) and limit
    all_signals.sort(key=lambda x: x.timestamp, reverse=True)

    return all_signals[:limit]

@app.get("/api/v1/signals/{symbol}")
async def get_signals(symbol: str):
    """Get signals for specific symbol"""
    logger.info(f"DEBUG: get_signals called with symbol={symbol}")
    signals = await generate_signals_cached(symbol)
    return signals

@app.get("/api/v1/market-data/{symbol}")
async def get_market_data(symbol: str):
    """Get market data for symbol"""
    if not symbol or len(symbol) > 10 or not symbol.isalnum():
        raise HTTPException(status_code=400, detail=f"Invalid symbol: {symbol}")

    data = await get_market_data_cached(symbol)
    if not data:
        raise HTTPException(status_code=404, detail=f"Market data not found for {symbol}")

    return data

@app.get("/api/v1/market-data/{symbol}/historical")
async def get_historical_data(
    symbol: str,
    period: str = Query("1mo", pattern="^(1d|5d|1mo|3mo|6mo|1y|2y|5y|10y|ytd|max)$"),
    interval: str = Query("1d", pattern="^(1m|2m|5m|15m|30m|60m|90m|1h|1d|5d|1wk|1mo|3mo)$")
):
    """Get historical data"""
    data = await get_historical_data_cached(symbol, period, interval)

    if data is None:
        return {"data": []}

    # Convert to JSON-serializable format with the correct field names for the chart
    records = []
    for index, row in data.iterrows():
        records.append({
            "time": int(index.timestamp()),  # Chart expects 'time' field in seconds
            "open": float(row["Open"]) if "Open" in row else 0,
            "high": float(row["High"]) if "High" in row else 0,
            "low": float(row["Low"]) if "Low" in row else 0,
            "close": float(row["Close"]) if "Close" in row else 0,
            "volume": int(row["Volume"]) if "Volume" in row else 0
        })

    return {"data": records}

@app.get("/api/v1/performance")
async def get_performance_stats():
    """Get performance statistics"""
    stats = {}
    for endpoint, times in request_times.items():
        if times:
            stats[endpoint] = {
                'count': len(times),
                'avg_ms': round(np.mean(times) * 1000, 2),
                'max_ms': round(max(times) * 1000, 2),
                'p95_ms': round(np.percentile(times, 95) * 1000, 2) if len(times) > 10 else 0
            }

    # Add cache statistics
    cache_stats = {
        'market_data_cache_size': len(market_data_cache),
        'signal_cache_size': len(signal_cache),
        'historical_cache_size': len(historical_cache),
        'total_requests': sum(len(times) for times in request_times.values())
    }

    return {
        "endpoints": stats,
        "cache": cache_stats,
        "uptime": time.time() - startup_time if 'startup_time' in globals() else 0
    }

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint with batching"""
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            # Handle any client messages if needed
    except WebSocketDisconnect:
        manager.disconnect(websocket)

@app.websocket("/ws/signals")
async def websocket_signals_endpoint(websocket: WebSocket):
    """WebSocket endpoint for signal updates with batching"""
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            # Handle any client messages if needed
            # Parse and handle subscription messages
            try:
                message = json.loads(data)
                if message.get('type') == 'system' and message.get('action') == 'handshake':
                    # Send handshake response
                    await websocket.send_json({
                        "type": "system",
                        "action": "handshake_response",
                        "data": {
                            "server_type": "mock_backend",
                            "version": "2.0.0",
                            "capabilities": ["signals", "market_data"]
                        },
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    })
                elif message.get('type') == 'subscribe':
                    # Handle subscription
                    await websocket.send_json({
                        "type": "subscription",
                        "action": "confirmed",
                        "topic": message.get('topic'),
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    })
            except json.JSONDecodeError:
                pass
    except WebSocketDisconnect:
        manager.disconnect(websocket)

@app.get("/api/v1/signals/{symbol}/insights")
async def get_signal_insights(symbol: str):
    """Get insights for a symbol using cached data"""
    signals = await generate_signals_cached(symbol)

    if not signals:
        return {
            "symbol": symbol,
            "signal_count": 0,
            "recommendation": "NO_DATA"
        }

    # Aggregate signals
    buy_signals = sum(1 for s in signals if s.action == "BUY")
    sell_signals = sum(1 for s in signals if s.action == "SELL")
    avg_confidence = np.mean([s.confidence for s in signals])

    recommendation = "HOLD"
    if buy_signals > sell_signals:
        recommendation = "BUY"
    elif sell_signals > buy_signals:
        recommendation = "SELL"

    return {
        "symbol": symbol,
        "signal_count": len(signals),
        "buy_signals": buy_signals,
        "sell_signals": sell_signals,
        "average_confidence": round(avg_confidence, 2),
        "recommendation": recommendation,
        "latest_signal": signals[0].model_dump() if signals else None
    }

@app.get("/api/v1/market/opportunities")
async def get_market_opportunities():
    """Get market opportunities using batch processing"""
    symbols = ["AAPL", "GOOGL", "MSFT", "TSLA", "SPY"]

    # Get signals for all symbols
    all_signals = await generate_signals_batch(symbols)

    # Filter high confidence signals
    opportunities = [
        s for s in all_signals
        if s.confidence > 0.7 and s.action in ["BUY", "SELL"]
    ]

    # Sort by confidence
    opportunities.sort(key=lambda x: x.confidence, reverse=True)

    return {"opportunities": [opp.model_dump() for opp in opportunities[:10]]}

# Precise options endpoint (cached)
@app.get("/api/v1/signals/precise-options")
async def get_precise_options_signals(
    symbol: str = Query(..., description="Stock symbol"),
    timeframe: str = Query("15m", pattern="^(1m|5m|15m|30m|1h|4h|1d)$")
):
    """Get precise options signals"""
    # For now, return regular signals (can be enhanced with options-specific logic)
    signals = await generate_signals_cached(symbol)

    # Add options-specific fields
    for signal in signals:
        signal.indicators["options_volume"] = random.randint(1000, 50000)
        signal.indicators["implied_volatility"] = round(random.uniform(0.2, 0.8), 2)
        signal.indicators["options_flow"] = random.choice(["bullish", "bearish", "neutral"])

    return signals

# New API endpoints for signal engine and pipeline management
@app.get("/api/v1/pipeline/stats")
async def get_pipeline_stats():
    """Get signal filtering pipeline statistics"""
    if not signal_pipeline:
        return {"error": "Signal pipeline not available"}

    return signal_pipeline.get_pipeline_stats()

@app.post("/api/v1/pipeline/configure")
async def configure_pipeline(config: Dict[str, Any]):
    """Configure the signal filtering pipeline"""
    if not signal_pipeline:
        return {"error": "Signal pipeline not available"}

    try:
        # Apply configuration
        for filter_obj in signal_pipeline.filters:
            filter_name = filter_obj.name.lower()

            if "confidence_filter" in filter_name and "min_confidence" in config:
                filter_obj.min_confidence = config["min_confidence"]
            elif "quality" in filter_name and "min_quality_score" in config:
                filter_obj.min_quality_score = config["min_quality_score"]
            elif "risk" in filter_name and "allowed_risk_levels" in config:
                filter_obj.allowed_risk_levels = config["allowed_risk_levels"]
            elif "volume" in filter_name and "min_volume_ratio" in config:
                filter_obj.min_volume_ratio = config["min_volume_ratio"]

        return {"status": "configured", "config": config}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/api/v1/signals/quality-report")
async def get_signal_quality_report():
    """Get comprehensive signal quality report"""
    if not signal_engine or not signal_pipeline:
        return {"error": "Signal engine not available"}

    # Generate signals for test symbols
    test_symbols = ["AAPL", "GOOGL", "MSFT", "SPY"]
    all_trading_signals = await signal_engine.generate_signals(test_symbols)
    filtered_signals = signal_pipeline.filter_signals(all_trading_signals)

    report = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "total_signals_generated": len(all_trading_signals),
        "signals_after_filtering": len(filtered_signals),
        "filter_rate": 1 - (len(filtered_signals) / max(1, len(all_trading_signals))),
        "pipeline_stats": signal_pipeline.get_pipeline_stats(),
        "signal_breakdown": {
            "by_action": {},
            "by_risk_level": {},
            "by_confidence": {
                "high": 0,
                "medium": 0,
                "low": 0
            }
        }
    }

    # Analyze filtered signals
    for signal in filtered_signals:
        # By action
        report["signal_breakdown"]["by_action"][signal.action] = \
            report["signal_breakdown"]["by_action"].get(signal.action, 0) + 1

        # By risk level
        report["signal_breakdown"]["by_risk_level"][signal.risk_level] = \
            report["signal_breakdown"]["by_risk_level"].get(signal.risk_level, 0) + 1

        # By confidence
        if signal.confidence >= 0.8:
            report["signal_breakdown"]["by_confidence"]["high"] += 1
        elif signal.confidence >= 0.6:
            report["signal_breakdown"]["by_confidence"]["medium"] += 1
        else:
            report["signal_breakdown"]["by_confidence"]["low"] += 1

    return report

@app.post("/api/v1/signals/feedback")
async def submit_signal_feedback(
    signal_id: str,
    outcome: str = Query(..., pattern="^(success|failure|partial)$"),
    profit_loss: Optional[float] = None,
    notes: Optional[str] = None
):
    """Submit feedback for a signal to improve future performance"""
    # For now, just log the feedback
    # In future, this can be stored and used to train the ML model
    feedback = {
        "signal_id": signal_id,
        "outcome": outcome,
        "profit_loss": profit_loss,
        "notes": notes,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

    logger.info(f"Signal feedback received: {feedback}")

    # If pipeline is available, adjust parameters based on feedback
    if signal_pipeline and outcome == "failure":
        # Increase filtering strictness on failures
        performance_metrics = {
            "false_positive_rate": 0.4,  # Simulated
            "signal_accuracy": 0.4  # Simulated
        }
        signal_pipeline.adjust_filter_parameters(performance_metrics)

    return {"status": "feedback_received", "feedback": feedback}

# Monitoring service endpoints
@app.post("/api/v1/monitoring/track-entry")
async def track_signal_entry(signal_data: Dict[str, Any]):
    """Track when a signal is acted upon"""
    if not monitoring_service:
        return {"error": "Monitoring service not available"}

    monitoring_service.track_signal_entry(signal_data)
    return {"status": "tracking_started", "signal_id": signal_data.get('id')}

@app.post("/api/v1/monitoring/track-exit")
async def track_signal_exit(
    signal_id: str,
    exit_price: float,
    outcome: str = Query("success", pattern="^(success|failure|partial)$"),
    notes: Optional[str] = None
):
    """Track when a signal position is closed"""
    if not monitoring_service:
        return {"error": "Monitoring service not available"}

    monitoring_service.track_signal_exit(signal_id, exit_price, outcome, notes or "")
    return {"status": "exit_tracked", "signal_id": signal_id}

@app.get("/api/v1/monitoring/performance")
async def get_performance_metrics(
    timeframe_days: Optional[int] = None,
    symbol: Optional[str] = None
):
    """Get comprehensive performance metrics"""
    if not monitoring_service:
        return {"error": "Monitoring service not available"}

    timeframe = timedelta(days=timeframe_days) if timeframe_days else None
    metrics = monitoring_service.get_performance_metrics(timeframe, symbol)
    return metrics.to_dict()

@app.get("/api/v1/monitoring/recommendations")
async def get_improvement_recommendations():
    """Get recommendations for improving signal generation"""
    if not monitoring_service:
        return {"error": "Monitoring service not available"}

    recommendations = monitoring_service.generate_improvement_recommendations()
    return {"recommendations": recommendations}

@app.get("/api/v1/monitoring/feedback-summary")
async def get_feedback_summary():
    """Get summary of signal feedback"""
    if not monitoring_service:
        return {"error": "Monitoring service not available"}

    return monitoring_service.get_signal_feedback_summary()

@app.post("/api/v1/monitoring/snapshot")
async def save_performance_snapshot():
    """Save a performance snapshot"""
    if not monitoring_service:
        return {"error": "Monitoring service not available"}

    monitoring_service.save_performance_snapshot()
    return {"status": "snapshot_saved", "timestamp": datetime.now(timezone.utc).isoformat()}

@app.get("/api/v1/monitoring/active-signals")
async def get_active_signals():
    """Get currently active (pending) signals"""
    if not monitoring_service:
        return {"error": "Monitoring service not available"}

    active_signals = [
        {**outcome.to_dict(), 'signal_id': signal_id}
        for signal_id, outcome in monitoring_service.active_signals.items()
    ]
    return {"active_signals": active_signals, "count": len(active_signals)}

# Backtesting endpoints
@app.post("/api/v1/backtest/run")
async def run_backtest(
    symbols: List[str] = Query(..., description="List of symbols to backtest"),
    start_date: str = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: str = Query(None, description="End date (YYYY-MM-DD)"),
    quick_mode: bool = Query(False, description="Run quick backtest with reduced data")
):
    """Run ML-enhanced backtest with signal quality metrics"""
    try:
        from ml_enhanced_backtest_system import MLBacktestEngine

        engine = MLBacktestEngine()

        # Use default dates if not provided
        if not end_date:
            end_date = datetime.now(tz=timezone.utc).strftime('%Y-%m-%d')
        if not start_date:
            # Quick mode: 3 months, normal: 1 year
            days_back = 90 if quick_mode else 365
            start_date = (datetime.now(tz=timezone.utc) - timedelta(days=days_back)).strftime('%Y-%m-%d')

        # Run backtest
        results = await engine.run_comprehensive_backtest(
            symbols=symbols[:5],  # Limit to 5 symbols
            start_date=start_date,
            end_date=end_date
        )

        # Format results for API response
        formatted_results = {}
        for symbol, data in results.items():
            formatted_results[symbol] = {
                "performance": {
                    "annual_return": data['backtest_metrics']['annual_return'],
                    "sharpe_ratio": data['backtest_metrics']['sharpe_ratio'],
                    "max_drawdown": data['backtest_metrics']['max_drawdown'],
                    "win_rate": data['backtest_metrics']['win_rate'],
                    "profit_factor": data['backtest_metrics']['profit_factor']
                },
                "ml_accuracy": data['avg_accuracy'],
                "signal_quality": data.get('signal_quality', {}),
                "top_features": data['feature_importance'][:5]
            }

        return {
            "status": "success",
            "backtest_period": f"{start_date} to {end_date}",
            "results": formatted_results
        }

    except Exception as e:
        logger.error(f"Error running backtest: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/backtest/recommendations")
async def get_backtest_recommendations(
    symbols: List[str] = Query(..., description="Symbols to analyze")
):
    """Get signal improvement recommendations from backtesting"""
    try:
        from ml_enhanced_backtest_system import SignalAccuracyImprover

        improver = SignalAccuracyImprover()
        improvements = await improver.improve_signals(symbols[:5])

        return {
            "recommended_features": improvements['recommended_features'][:10],
            "optimal_parameters": improvements['optimal_parameters'],
            "risk_management": improvements['risk_management'],
            "signal_filters": improvements['signal_filters']
        }

    except Exception as e:
        logger.error(f"Error getting recommendations: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/logs/frontend")
async def log_frontend_message(request: Request):
    """Handle frontend logging messages"""
    try:
        body = await request.json()
        log_level = body.get('level', 'info')
        message = body.get('message', '')
        data = body.get('data', {})

        # Filter out empty or repetitive messages
        if not message.strip() or (not data or data == {}):
            return {"status": "ignored", "timestamp": datetime.now(timezone.utc).isoformat()}

        # Only log important messages (warnings and errors)
        if log_level.lower() in ['warn', 'warning', 'error', 'critical']:
            logger.info(f"Frontend [{log_level.upper()}]: {message} - {data}")

        return {"status": "logged", "timestamp": datetime.now(timezone.utc).isoformat()}
    except Exception as e:
        error_tracker.track_error(e, {
            'endpoint': '/api/logs/frontend',
            'operation': 'log_frontend_message'
        })
        logger.error(f"Error logging frontend message: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/api/v1/monitoring/errors")
async def get_error_summary():
    """Get comprehensive error tracking summary"""
    try:
        return error_tracker.get_error_summary()
    except Exception as e:
        error_tracker.track_error(e, {
            'endpoint': '/api/v1/monitoring/errors',
            'operation': 'get_error_summary'
        })
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/monitoring/performance-detailed")
async def get_performance_summary():
    """Get comprehensive performance monitoring summary"""
    try:
        return performance_monitor.get_performance_summary()
    except Exception as e:
        error_tracker.track_error(e, {
            'endpoint': '/api/v1/monitoring/performance-detailed',
            'operation': 'get_performance_summary'
        })
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/monitoring/health")
async def get_system_health():
    """Get comprehensive system health status"""
    try:
        error_summary = error_tracker.get_error_summary()
        performance_summary = performance_monitor.get_performance_summary()

        # Determine health status
        recent_errors = error_summary['recent_errors_1h']
        total_requests = performance_summary['total_requests']

        if recent_errors > 50:
            health_status = "critical"
        elif recent_errors > 20:
            health_status = "warning"
        elif recent_errors > 5:
            health_status = "degraded"
        else:
            health_status = "healthy"

        return {
            "status": health_status,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "uptime": time.time() - start_time if 'start_time' in globals() else 0,
            "errors": error_summary,
            "performance": performance_summary,
            "cache_stats": {
                "market_data_cache": {
                    "size": len(market_data_cache),
                    "maxsize": market_data_cache.maxsize
                },
                "signal_cache": {
                    "size": len(signal_cache),
                    "maxsize": signal_cache.maxsize
                },
                "historical_cache": {
                    "size": len(historical_cache),
                    "maxsize": historical_cache.maxsize
                }
            }
        }
    except Exception as e:
        error_tracker.track_error(e, {
            'endpoint': '/api/v1/monitoring/health',
            'operation': 'get_system_health'
        })
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/monitoring/clear-errors")
async def clear_error_history():
    """Clear error history (useful for testing or after resolving issues)"""
    try:
        error_tracker.errors.clear()
        error_tracker.error_counts.clear()
        error_tracker.error_patterns.clear()
        return {"status": "cleared", "timestamp": datetime.now(timezone.utc).isoformat()}
    except Exception as e:
        error_tracker.track_error(e, {
            'endpoint': '/api/v1/monitoring/clear-errors',
            'operation': 'clear_error_history'
        })
        raise HTTPException(status_code=500, detail=str(e))

# Record startup time for uptime tracking
startup_time = time.time()

if __name__ == "__main__":
    try:
        print("ðŸš€ Starting GoldenSignalsAI Optimized Backend")
        print("ðŸ“Š Caching enabled for improved performance")
        print("ðŸ”„ Batch processing for concurrent requests")
        print("ðŸ“ˆ Performance monitoring at /api/v1/monitoring/performance-detailed")
        print("ðŸ” Error tracking at /api/v1/monitoring/errors")
        print("ðŸ¥ Health monitoring at /api/v1/monitoring/health")
        print("ðŸŒ API docs available at http://localhost:8000/docs")

        uvicorn.run(app, host="0.0.0.0", port=8000)
    except Exception as e:
        error_tracker.track_error(e, {
            'operation': 'server_startup',
            'context': 'main'
        })
        logger.critical(f"Failed to start server: {e}")
        raise
